{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biểu thức hàm mất mát\n",
    "$$L = -\\log(P(w_c|w_t)) - \\sum_{n \\in N_{t,c}}(1 - P(w_n|w_t)) = \\log \\left( 1 + e^{-s(w_t, w_c)} \\right) + \\sum_{n \\in N_{t,c}} \\log \\left( 1 + e^{s(w_t, n)} \\right)$$\n",
    "$$s(w, c) = u_t^{\\top} v_c = \\sum_{g \\in \\mathcal{G}_w} z_g^{\\top} v_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Đạo hàm hàm mất mát (ví dụ theo $u_t$)\n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial u_t} = -v_c + \\dfrac{v_c}{1 + \\exp(-u_t^Tv_c)} + \\sum_{n \\in N_{t,c}}{(\\dfrac{v_n}{1 + exp(-u_t^Tv_n)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biểu thức xác suất\n",
    "$$P((w_c | w_t) = \\frac{e^{s(w_t, w_c)}}{\\sum_{j=1}^{W} e^{s(w_t, j)}}\\ \\approx P(w_c | w_t) = \\frac{1}{1 + \\exp\\left(-u_t^{\\top} v_c\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random \n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(s):\n",
    "    s = s.lower()\n",
    "    cleaned_string = re.sub(r'[^a-zA-Z0-9 ]+', '', s)\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    return cleaned_string\n",
    "\n",
    "def pre_processing(corpus):\n",
    "    sentences = []\n",
    "    for sentence in corpus:\n",
    "        sentences.append((string_to_list(sentence)))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def generate_ngrams(word, n = 3):\n",
    "    word = f\"<{word}>\"\n",
    "    return [word[i : i + n] for i in range(len(word) - n + 1)]\n",
    "def build_vocab(corpus, n = 3):\n",
    "    vocab = defaultdict(int)\n",
    "    word_to_ngrams = {}\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            ngrams = generate_ngrams(word)\n",
    "            word_to_ngrams[word] = ngrams\n",
    "            for ngram in ngrams:\n",
    "                vocab[ngram] += 1\n",
    "    return {ngram : idx for idx, ngram in enumerate(vocab)}, word_to_ngrams\n",
    "def fasttext_loss(v_w, v_c, negative_vectors):\n",
    "    positive_score = np.dot(v_w, v_c)\n",
    "    positive_loss = np.log(sigmoid(positive_score))\n",
    "\n",
    "    negative_loss = 0\n",
    "    for negative_vector in negative_vectors:\n",
    "        negative_score = np.dot(v_w, negative_vector)\n",
    "        negative_loss += np.log(sigmoid(-negative_score))\n",
    "    return -(positive_loss + negative_loss)\n",
    "def update_vectors(v_w, v_c, negative_vectors, lr = 0.025):\n",
    "    grad_positive = sigmoid(np.dot(v_w, v_c)) - 1\n",
    "    grad_v_w = lr * grad_positive * v_c\n",
    "    v_c -= lr * grad_positive * v_w\n",
    "    for negative_vector in negative_vectors:\n",
    "        grad_negative = sigmoid(np.dot(v_w, negative_vector))\n",
    "        grad_v_w += lr * grad_negative * negative_vector\n",
    "        negative_vector -= lr * grad_negative * v_w\n",
    "    v_w -= grad_v_w\n",
    "    return v_w, v_c, negative_vectors\n",
    "# def update_vectors(v_w, v_c, negative_vectors, lr = 0.025):\n",
    "#     grad_positive = sigmoid(np.dot(v_w, v_c)) - 1\n",
    "#     v_w -= lr * grad_positive * v_c\n",
    "#     v_c -= lr * grad_positive * v_w\n",
    "\n",
    "#     for negative_vector in negative_vectors:\n",
    "#         grad_negative = sigmoid(np.dot(v_w, negative_vector))\n",
    "#         negative_vector -= lr * grad_negative * v_w\n",
    "#         v_w -= lr * grad_negative * negative_vector\n",
    "#     return v_w, v_c, negative_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext(corpus, vocab, vector_size = 300, window_size = 2, lr = 0.025, epochs = 100, negative_samples = 5):\n",
    "    vocab_size = len(vocab)\n",
    "    u_vectors = np.random.uniform(-0.5, 0.5, (vocab_size, vector_size))\n",
    "    v_vectors = np.random.uniform(-0.5, 0.5, (vocab_size, vector_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for sentence in corpus:\n",
    "            words = sentence.split()\n",
    "            for i, word in enumerate(words):\n",
    "                ngrams = generate_ngrams(word)\n",
    "                word_idx = [vocab[ngram] for ngram in ngrams if ngram in vocab]\n",
    "                for j in range(max(i - window_size, 0), min(i + window_size, len(words))):\n",
    "                    if i != j:\n",
    "                        context_word = words[j]\n",
    "                        context_ngrams = generate_ngrams(context_word)\n",
    "                        context_word_idx = [vocab[ngram] for ngram in context_ngrams if ngram in vocab]\n",
    "                        negative_indices = [random.randint(0, vocab_size - 1) for _ in range(negative_samples)]\n",
    "\n",
    "                        for word_id in word_idx:\n",
    "                            for context_id in context_word_idx:\n",
    "                                v_w = u_vectors[word_id]\n",
    "                                v_c = v_vectors[context_id]\n",
    "                                negative_vectors = [u_vectors[neg_id] for neg_id in negative_indices]\n",
    "                                u_vectors[word_id], v_vectors[context_id], _ = update_vectors(v_w, v_c, negative_vectors, lr = lr)\n",
    "        \n",
    "    return u_vectors, v_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(word, u_vectors, word_to_ngrams, vocab, top_n = 5):\n",
    "    word_ngrams = generate_ngrams(word)\n",
    "    word_indices = [vocab[ngram] for ngram in word_ngrams if ngram in vocab]\n",
    "    if not len(word_indices):\n",
    "        print(\"Word not found in vocab!\")\n",
    "        return []\n",
    "    word_vector = np.mean([u_vectors[idx] for idx in word_indices], axis = 0)\n",
    "    similarities = {}\n",
    "    for other_word, ngrams in word_to_ngrams.items():\n",
    "        ngram_indices = [vocab[ngram] for ngram in ngrams if ngram in vocab]\n",
    "        if len(ngram_indices) > 0:\n",
    "            other_word_vector = np.mean([u_vectors[idx] for idx in ngram_indices], axis = 0)\n",
    "            similarity = np.dot(word_vector, other_word_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(other_word_vector))\n",
    "            similarities[other_word] = similarity\n",
    "    similar_words = sorted(similarities.items(), key = lambda item : item[1], reverse=True)[:top_n]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n"
     ]
    }
   ],
   "source": [
    "# Tập dữ liệu\n",
    "file_path = r\".\\FastText.xlsx\"\n",
    "df = pd.read_excel(file_path, header = None)\n",
    "df = df.dropna()\n",
    "corpus = pre_processing(df[0].tolist())\n",
    "\n",
    "# Xây dựng từ điển n-gram và liên kết từ với các n-gram\n",
    "vocab, word_to_ngrams = build_vocab(corpus)\n",
    "\n",
    "# Huấn luyện mô hình FastText\n",
    "u_vectors, v_vectors = train_fasttext(corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peace: 0.7034\n",
      "really: 0.4339\n",
      "who: 0.1998\n",
      "simply: 0.1972\n",
      "onward: 0.1714\n"
     ]
    }
   ],
   "source": [
    "similar_words = find_similar(\"peacefully\", u_vectors, word_to_ngrams, vocab)\n",
    "\n",
    "# In ra các từ gần nghĩa nhất\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
